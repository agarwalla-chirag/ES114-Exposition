[
  {
    "objectID": "blog.html",
    "href": "blog.html",
    "title": "Sympy Stats",
    "section": "",
    "text": "Sympy Stats\n\n\n\n\n\n\nStats\n\n\n\n\n\n\n\n\n\nFeb 25, 2025\n\n\nChirag Agarwalla, Niraj Kumar, Nishanth P Hegde\n\n\n\n\n\n\nNo matching items"
  },
  {
    "objectID": "posts/Sympy_Stats_Module.html",
    "href": "posts/Sympy_Stats_Module.html",
    "title": "Sympy Stats",
    "section": "",
    "text": "Introduction\nSymPy Stats is a powerful module in the SymPy library that provides symbolic tools for probability and statistics in Python. It enables the algebraic manipulation of random variables, making it perfect for probabilistic modeling and statistical analysis.\nIn this blog, we‚Äôll explore the key features of SymPy Stats, including how to define random variables, compute probabilities, and analyze statistical properties such as expectation, variance, and more. Additionally, we‚Äôll dive into important concepts like conditional probability, independence, and other advanced topics in symbolic statistics.\n\n\nInstallation and Setup\nTo use SymPy Stats, ensure that SymPy is installed. It can be installed using:\npip install sympy\nNow, import the necessary modules:\n\nfrom sympy.stats import *\nfrom sympy import symbols, Eq, simplify\nimport matplotlib.pyplot as plt\nimport numpy as np\n\n\n\nKey Features and Explanations with Code Examples\nSymPy Stats provides functionalities for:\n\n\nDefining Random Variables and Probablity Computation\nStandard Normal Distribution\nWe define a random variable ‚ÄòNorm‚Äô that follows a normal distribution with a mean (ùúá) = 0 and standard deviation œÉ=1, which makes it a standard normal variable.\nThen, P(Norm&gt;0) calculates the probability that ‚ÄòNorm‚Äô is greater than 0. This is the area under the normal curve to the right of 0.\n\nNorm = Normal('Norm', 0, 1)\n\nprint(\"The probablity of normal distribution is:\")\nP(Norm &gt; 0)\n\nThe probablity of normal distribution is:\n\n\n\\(\\displaystyle \\frac{1}{2}\\)\n\n\nExponential Distribution\nWe define a random variable ‚ÄòExp‚Äô that follows an exponential distribution with a rate parameter Œª=1, which makes it an exponential variable with mean Œº=1.\nFor a continuous random variable, calculating P(Exp=1) gives the probability that ‚ÄòExp‚Äô is exactly equal to 1. However, since the exponential distribution is continuous, the probability of ‚ÄòExp‚Äô being exactly 1 is 0.\nOn the other hand, P(0&lt;Exp&lt;1) calculates the probability that ‚ÄòExp‚Äô lies between 0 and 1. This is the area under the curve between 0 and 1.\n\nExp = Exponential('Exp', 1)\n\nprint(\"The probability Exp = 1 is:\")\nP(Eq(Exp, 1))\n\nThe probability Exp = 1 is:\n\n\n\\(\\displaystyle 0\\)\n\n\n\nprint(\"The probability that 0 &lt; Exp &lt; 1 is:\")\nP((Exp &gt; 0) & (Exp &lt; 1))\n\nThe probability that 0 &lt; Exp &lt; 1 is:\n\n\n\\(\\displaystyle 1 - e^{-1}\\)\n\n\nUniform Distribution\nWe define a random variable ‚ÄòUni‚Äô that follows a uniform distribution between 0 and 10, meaning it has equal probability across this range.\nThen, P(Uni&lt;5) calculates the probability that ‚ÄòUni‚Äô is less than 5. This probability is proportional to the length of the interval [0,5] relative to the total range [0,10], representing the area under the uniform curve from 0 to 5.\n\nUni = Uniform('Uni', 0, 10)\n\nprint(\"The probability that Uni &lt; 5 is:\")\nP(Uni &lt; 5)\n\nThe probability that Uni &lt; 5 is:\n\n\n\\(\\displaystyle \\frac{1}{2}\\)\n\n\nBernoulli Distribution\nWe define a random variable ‚ÄòBern‚Äô that follows a Bernoulli distribution with probability p=0.5. It has two outcomes: 1 (success) with probability ‚Äòùëù‚Äô, and 0 (failure) with probability ‚Äò1‚àíp‚Äô.\nThen, ùëÉ(Eq(Bern,1)) calculates the probability that ‚ÄòBern‚Äô equals 1. This represents the probability of success in a Bernoulli trial.\n\nBern = Bernoulli('Bern', 0.5)\n\nprint(\"The probability that Bern = 1 is:\")\nP(Eq(Bern, 1))\n\nThe probability that Bern = 1 is:\n\n\n\\(\\displaystyle 0.5\\)\n\n\nBinomial Distribution\nWe define a random variable ‚ÄòBin‚Äô that follows a Binomial distribution with parameters n=10 and p=0.5, meaning there are 10 independent Bernoulli trials, each with a probability p=0.5 of success.\nThen, P(Eq(Bin,5)) calculates the probability that ‚ÄòBin‚Äô equals 5, which represents the probability of having exactly 5 successes out of the 10 trials and is computed using the Binomial probability mass function.\n\nBin = Binomial('Bin', 10, 0.5)\n\nprint(\"The probability that Bin = 5 is:\")\nP(Eq(Bin, 5))\n\nThe probability that Bin = 5 is:\n\n\n\\(\\displaystyle 0.24609375\\)\n\n\nPoisson Distribution\nWe define a random variable ‚ÄòPoiss‚Äô that follows a Poisson distribution with a mean Œª=3.\nThen, P(Eq(Poiss,2)) calculates the probability that ‚ÄôPoiss‚Äôequals 2, which represents the probability of exactly 2 events occurring in the given interval and is computed using the Poisson probability mass function.\nThis gives the likelihood of observing exactly 2 events when the average number of events is 3.\n\nPoiss = Poisson('Poiss', 3)\n\nprint(\"The probability that Poiss = 2 is:\")\nP(Eq(Poiss, 2))\n\nThe probability that Poiss = 2 is:\n\n\n\\(\\displaystyle \\frac{9}{2 e^{3}}\\)\n\n\n\n\nExpectation and Variance\n\nExpectation (E): The expectation, or mean, of a random variable represents its average value over many trials. It gives the central tendency of the distribution.\nVariance: Variance measures how much the values of a random variable deviate from the mean. A higher variance indicates a wider spread, while a lower variance means the values are closer to the mean.\n\nFor a normal distribution, the expectation is the mean (Œº), and the variance is the square of the standard deviation (œÉ¬≤).\n\nprint(\"The expectation and variance of normal distribution is:\")\nE(Norm), variance(Norm)\n\nThe expectation and variance of normal distribution is:\n\n\n(0, 1)\n\n\n\nprint(\"The expectation and variance of exponential distribution is:\")\nE(Exp), variance(Exp)\n\nThe expectation and variance of exponential distribution is:\n\n\n(1, 1)\n\n\n\nprint(\"The expectation and variance of uniform distribution is:\")\nE(Uni) ,variance(Uni)\n\nThe expectation and variance of uniform distribution is:\n\n\n(5, 25/3)\n\n\n\nprint(\"The expectation and variance of Bernoulli distribution is:\")\nE(Bern), variance(Bern)\n\nThe expectation and variance of Bernoulli distribution is:\n\n\n(0.500000000000000, 0.250000000000000)\n\n\n\nprint(\"The expectation and variance of Binomial distribution is:\")\nE(Bin), variance(Bin)\n\nThe expectation and variance of Binomial distribution is:\n\n\n(5.00000000000000, 2.50000000000000)\n\n\n\nprint(\"The expectation and variance of Poisson distribution is:\")\nE(Poiss)  , variance(Poiss)\n\nThe expectation and variance of Poisson distribution is:\n\n\n(3, 3)\n\n\n\n\nMoment\nMoments are statistical measures that help describe the shape and properties of a probability distribution.\nRaw Moments: The nth moment about the origin is given by E(X ^ n ) and helps understand the overall magnitude of the values.\nCentral Moments: Moments calculated about the mean describe variability and shape characteristics of the distribution.\nFirst Moment (Expectation): Represents the average or central value of the distribution.\nSecond Moment (Variance): Measures the spread of the distribution around the mean.\nThird Moment (Skewness): Describes the asymmetry of the distribution‚Äîwhether it leans more to the left or right.\nFourth Moment (Kurtosis): Indicates the heaviness of the tails compared to a normal distribution, helping assess the likelihood of extreme values.\nMoments offer a more detailed understanding of a distribution‚Äôs shape beyond just mean and variance.\n\nmoment(Norm, 3)  # Third moment of Normal Distribution\n\n\\(\\displaystyle 0\\)\n\n\n\nmoment(Exp, 4)  # Fourth moment of Exponential Distribution\n\n\\(\\displaystyle 24\\)\n\n\n\nmoment(Poiss, 2)  # Second moment of Poisson distribution\n\n\\(\\displaystyle 12\\)\n\n\n\nskewness(Exp)  # Skewness of Exponential Distribution\n\n\\(\\displaystyle 2\\)\n\n\n\nkurtosis(Exp)  # Kurtosis of Exponential Distribution\n\n\\(\\displaystyle 9\\)\n\n\n\n\nEntropy\nEntropy is a measure of uncertainty or randomness in a probability distribution. In this context, it quantifies the uncertainty associated with the standard normal distribution represented by the variable Norm.\nH(X): This line calls the function H() from sympy.stats, which is used to calculate the entropy of a random variable.\n\nH(Norm)\n\n\\(\\displaystyle \\frac{\\log{\\left(2 \\right)}}{2} + \\frac{1}{2} + \\frac{\\log{\\left(\\pi \\right)}}{2}\\)\n\n\nThis line of code calculates the entropy of an exponential distribution.\n\nH(Exp)\n\n\\(\\displaystyle 1\\)\n\n\nThis line of code calculates the entropy of a Bernoulli random variable.\n\nH(Bern)\n\n\\(\\displaystyle 0.693147180559945\\)\n\n\n\n\nDensity\nThis is a function from sympy.stats that is used to obtain the probability density function (PDF) of a random variable. The PDF describes the likelihood of a random variable taking on a specific value. this line of code is asking for the mathematical formula that describes the shape of the standard normal distribution\n\ndensity(Norm)\n\n\\(\\displaystyle \\operatorname{NormalDistribution}\\left(0, 1\\right)\\)\n\n\n\ndensity(Exp)\n\n\\(\\displaystyle \\operatorname{ExponentialDistribution}\\left(1\\right)\\)\n\n\n\ndensity(Poiss)\n\n\\(\\displaystyle \\operatorname{PoissonDistribution}\\left(3\\right)\\)\n\n\n\n\nCumulative Distribution Function\nThe cdf(Exp) line is calculating this probability for the exponential distribution represented by Exp. The exponential distribution is often used to model the time until an event occurs.\n\ncdf(Exp)\n\n\\(\\displaystyle \\left( z \\mapsto \\begin{cases} 1 - e^{- z} & \\text{for}\\: z \\geq 0 \\\\0 & \\text{otherwise} \\end{cases} \\right)\\)\n\n\nThe cdf of Bernoulli Distribution can be shown as:\n\ncdf(Bern)\n\n{0: 0.500000000000000, 1: 1.00000000000000}\n\n\n\n\nQuantile\nThe code is asking for the value (on the x-axis of the standard normal distribution) below which 95% of the data falls. This is also known as the 95th percentile or 0.95 quantile. Imagine the bell curve of the standard normal distribution ‚Äì this code finds the point where 95% of the area under the curve is to the left of that point\n\nquantile(Norm, 0.95)\n\n\\(\\displaystyle \\left( p \\mapsto \\sqrt{2} \\operatorname{erfinv}{\\left(2 p - 1 \\right)} \\right)\\)\n\n\n\nquantile(Exp, 0.95)\n\n\\(\\displaystyle \\left( p \\mapsto - \\log{\\left(1 - p \\right)} \\right)\\)\n\n\n\n\nConditional Probability\nThis code snippet calculates the probability that a random variable, represented by Norm, falls within a specific range given that it‚Äôs part of a standard normal distribution.\n\n\n\nP(Norm &gt; 0, Norm &lt; 2)\n\n\\(\\displaystyle - \\frac{\\sqrt{2} \\pi e^{2} \\operatorname{erf}{\\left(\\sqrt{2} \\right)}}{- \\sqrt{2} \\pi e^{2} - \\sqrt{2} \\pi e^{2} \\operatorname{erf}{\\left(\\sqrt{2} \\right)}}\\)\n\n\nThis line calculates the probability that the random variable Exp (which was previously defined as an Exponential distribution) falls between the values of 2 and 5.\n\nP(Exp &gt; 2, Exp &lt; 5)\n\n\\(\\displaystyle - \\frac{1}{-1 + e^{5}} + \\frac{e^{3}}{-1 + e^{5}}\\)\n\n\nThis code calculates the probability that the random variable Bin is greater than 3 and less than 8.\n\nP(Bin &gt; 3, Bin &lt; 8)\n\n\\(\\displaystyle 0.818181818181818\\)\n\n\nThis code calculates the probability that the random variable Poiss is greater than 1 and less than 4.\n\nP(Poiss &gt; 1, Poiss &lt; 4)\n\n\\(\\displaystyle \\frac{9}{13}\\)\n\n\nThis code aims to find the likelihood of these two events occurring simultaneously:\nA randomly selected value from a standard normal distribution being greater than 0. A randomly selected value from an exponential distribution being greater than 1.\n\nP(Norm &gt; 0, Exp &gt; 1)\n\n\\(\\displaystyle \\frac{1}{2}\\)\n\n\n\n\nIndependence and Dependence of Random Variables\nIf the function returns True, it indicates that the variables are considered independent, meaning their outcomes don‚Äôt affect each other. If it returns False, it suggests there might be some dependence or relationship between the variables.\n\nindependent(Norm, Exp)\n\nTrue\n\n\n\nindependent(Bern, Bin)\n\nTrue\n\n\n\ndependent(Bern, Poiss)\n\nFalse\n\n\n\n\nCovariance and Correlation\nCovariance\nCovariance measures how two random variables change together. It is calculated as:\nCov(ùëã,ùëå)=ùê∏[(ùëã‚àíùê∏[ùëã])(ùëå‚àíùê∏[ùëå])]\n\n\nNorm_1 = Normal('Norm', 0, 1)  # Normal distribution with mean=0, std=1\nNorm_2 = Normal('Exp', 0, 2)  # Normal distribution with mean=0, std=2\n\ncov = covariance(Norm_1, Norm_2)\n\nprint(\"Covariance:\", cov)\n\nCovariance: 0\n\n\nCorrelation\nCorrelation normalizes covariance by the standard deviations of the variables, giving a value between -1 and 1:\nCorr(ùëã,ùëå)=Cov(ùëã,ùëå)/(ùúéùëãùúéùëå)\n\n\nNorm_1 = Normal('Norm', 0, 1)  # Normal distribution with mean=0, std=1\nNorm_2 = Normal('Exp', 0, 2)  # Normal distribution with mean=0, std=2\n\ncorr = correlation(Norm_1, Norm_2)\n\nprint(\"Correlation:\", corr)\n\nCorrelation: 0\n\n\nNote: 1. If X and Y are independent, covariance(X, Y) will return 0. 2. correlation(X, Y) normalizes the covariance to get a value between -1 and 1.\nCovariance between Binomial and Poisson\n\nBin = Binomial('Bin', n=10, p=0.5)  # Binomial(n=10, p=0.5)\nPoiss = Poisson('Poiss', 3)         # Poisson(Œª=3)\n\n\ncov = covariance(Bin, Poiss)\n\nprint(\"Covariance:\", cov)\n\nCovariance: 0\n\n\nNote: 1. If Bin and Poiss are independent, their covariance is 0. 2. By default, SymPy assumes independence, so covariance(Bin, Poiss) returns 0.\n\n\nTransformations of Random Variables\nIf X is a random variable and Y = g(X) is a transformation of X, then: * The new distribution of Y can be derived symbolically. * SymPy can compute the expectation, variance, and density function of Y.\nExample: Linear Transformation Y = aX + b\nIf X ~ N(0,1) (Standard Normal), and we define Y=2X+3, then:\nE[Y] = E[2X+3] = 2 E[X] + 3\nVar(Y) = Var(2X+3) = 2*2 Var(X) = 4\n\nNorm = Normal('Norm', 0, 1)\n\nTransformed_var = 2 * Norm + 3\n\nExpect = Expectation(Transformed_var).doit()\nVar = variance(Transformed_var).doit()\npdf_transformed_var = density(Transformed_var)(symbols('y'))\n\n\nprint(\"E[Transformed_var]:\", Expect)\nprint(\"Var[Transformed_var]:\", Var)\nprint(\"Density Function:\")\nsimplify(pdf_transformed_var)\n\nE[Transformed_var]: 3\nVar[Transformed_var]: 4\nDensity Function:\n\n\n\\(\\displaystyle \\frac{\\sqrt{2} e^{- \\frac{\\left(y - 3\\right)^{2}}{8}}}{4 \\sqrt{\\pi}}\\)\n\n\n\n\nSampling from Distributions\nSignificance of Sampling from a Distribution\nSampling from a probability distribution is important because it allows us to simulate, estimate, and analyze real-world data where randomness plays a role.\nFor Example:\nUnderstanding Random Behavior 1. When you sample from a Normal(0,1) distribution, for example, you get values mostly centered around 0 with some variation. 2. If you sample from a Poisson(Œª=3), the numbers tend to be small integers around 3. 3. This helps in understanding how real-world processes behave (e.g., height distribution, number of customer arrivals per minute).\nChecking Theoretical vs.¬†Empirical Distributions\nIf we sample a large number of values, we can plot a histogram and compare it with the expected probability distribution.\nExample: Sampling 1000 values from N(0,1) should give a bell-shaped curve.\nSampling from a Standard Normal Distribution\n\n#Normal distribution X ~ N(0,1)\nNorm = Normal('Norm', 0, 1)\n\nprint(\"Single Sample:\", round(sample(Norm),2))\n\n# Sample multiple values (Ex: 20 values)\nsamples = [round(sample(Norm),2) for _ in range(20)]\nprint(\"Multiple Samples:\", samples)\n\nSingle Sample: -1.53\nMultiple Samples: [np.float64(-0.2), np.float64(0.22), np.float64(-0.55), np.float64(-1.75), np.float64(-1.59), np.float64(1.27), np.float64(-0.05), np.float64(0.92), np.float64(2.54), np.float64(-0.36), np.float64(-0.09), np.float64(0.19), np.float64(2.09), np.float64(-0.01), np.float64(-0.86), np.float64(0.83), np.float64(0.47), np.float64(-0.68), np.float64(0.72), np.float64(-0.84)]\n\n\nSampling from a Binomial Distribution\n\n# X ~ Bin(10, 0.5)\nbin = Binomial('Bin', 10, 0.5)\n\nsamples = [sample(bin) for _ in range(20)]\nprint(\"Binomial Samples:\", samples)\n\nBinomial Samples: [np.int64(4), np.int64(3), np.int64(7), np.int64(5), np.int64(6), np.int64(8), np.int64(6), np.int64(5), np.int64(4), np.int64(4), np.int64(6), np.int64(5), np.int64(7), np.int64(7), np.int64(3), np.int64(3), np.int64(6), np.int64(6), np.int64(2), np.int64(5)]\n\n\nSampling from a Poisson Distribution\n\n# Poisson distribution X ~ Poisson(3)\nPoiss = Poisson('Poiss', 3)\n\nsamples = [sample(Poiss) for _ in range(20)]\nprint(\"Poisson Samples:\", samples)\n\nPoisson Samples: [np.int64(4), np.int64(2), np.int64(1), np.int64(3), np.int64(3), np.int64(4), np.int64(6), np.int64(5), np.int64(1), np.int64(4), np.int64(5), np.int64(3), np.int64(3), np.int64(0), np.int64(3), np.int64(3), np.int64(5), np.int64(4), np.int64(3), np.int64(2)]\n\n\n\n\nVisualization of Distributions\nNormal Distribution (N(0,1))\n\n#Normal distribution N(0,1)\nX_normal = Normal('X', 0, 1)\nx = symbols('x')\n\npdf_normal = density(X_normal)(x)\n\n# Plot the PDF of Normal Distribution\nx_vals = np.linspace(-4, 4, 1000)\ny_vals_normal = [pdf_normal.subs(x, val) for val in x_vals]\n\nplt.plot(x_vals, y_vals_normal, label='Normal N(0,1)', color='blue')\nplt.title('Normal Distribution N(0,1)')\nplt.xlabel('Value')\nplt.ylabel('Density')\nplt.legend()\nplt.grid(True)\nplt.show()\n\n\n\n\n\n\n\n\n\nThis is the Probability Density Function (PDF) for a Normal distribution with mean 0 and standard deviation 1.\nYou can see a bell-shaped curve, which is the characteristic of the normal distribution.\n\nBernoulli Distribution (Bernoulli(0.7))\n\n# Bernoulli distribution with p = 0.7\nX_bernoulli = Bernoulli('X', 0.7)\n\npmf_bernoulli = [density(X_bernoulli)(val).subs(x, val) for val in [0, 1]]\n\nplt.bar([0, 1], pmf_bernoulli, label='Bernoulli p=0.7', color='orange', alpha=0.6)\nplt.title('Bernoulli Distribution p=0.7')\nplt.xlabel('Outcome')\nplt.ylabel('Probability')\nplt.xticks([0, 1], ['Failure (0)', 'Success (1)'])\nplt.legend()\nplt.show()\n\n\n\n\n\n\n\n\nBinomial Distribution (Binomial(10, 0.5))\n\n# Binomial distribution Binomial(10, 0.5)\nX_binomial = Binomial('X', 10, 0.5)\n\nx_binomial = np.arange(0, 11)\npmf_binomial = [density(X_binomial)(val).subs(x, val) for val in x_binomial]\n\n\nplt.bar(x_binomial, pmf_binomial, label='Binomial Bin(10, 0.5)', color='red', alpha=0.6)\nplt.title('Binomial Distribution Bin(10, 0.5)')\nplt.xlabel('Number of Successes')\nplt.ylabel('Probability')\nplt.legend()\nplt.show()\n\n\n\n\n\n\n\n\nPoisson Distribution (Poisson(3))\n\nX_poisson = Poisson('X', 3)\n\nx_poisson = np.arange(0, 15)\npmf_poisson = [density(X_poisson)(val).subs(x, val) for val in x_poisson]\n\nplt.bar(x_poisson, pmf_poisson, label='Poisson Œª=3', color='purple', alpha=0.6)\nplt.title('Poisson Distribution Œª=3')\nplt.xlabel('Number of Events')\nplt.ylabel('Probability')\nplt.legend()\nplt.show()\n\n\n\n\n\n\n\n\n\n\nMore Examples\nExample 1: Coin Toss Probability A fair coin flip is modeled using a Bernoulli random variable, where:\n- Heads (1) occurs with probability 0.5\n- Tails (0) occurs with probability 0.5\nWe use SymPy Stats to define the random variable and calculate the probability of each outcome.\n\n# Define a fair coin toss (Heads = 1, Tails = 0)\n\nCoin = Bernoulli('Coin', 0.5)\nprint(\"Probability of Heads:\", P(Eq(Coin, 1)))\nprint(\"Probability of Tails:\", P(Eq(Coin, 0)))\n\nProbability of Heads: 0.500000000000000\nProbability of Tails: 0.500000000000000\n\n\nExample 2: Rolling a Dice Probability\nA fair 6-sided die is modeled using a Discrete Uniform distribution.\nWe calculate the probability of rolling 3 or higher, which means:\nP(Die &gt;= 3) = P(3) + P(4) + P(5) + P(6)\nThis example demonstrates symbolic probability calculations in SymPy Stats.\n\n# Define a fair 6-sided die\n\nDie = Die('Die', 6)\nprint(\"Probability of rolling 3 or higher:\", P(Die &gt;= 3))\n\nProbability of rolling 3 or higher: 2/3\n\n\nExample 3: Medical Test Accuracy\nProblem Statement A disease affects 1% of a population. A test for the disease:\n- Correctly detects the disease (True Positive) 95% of the time\n- Incorrectly flags a healthy person as positive (False Positive) 5% of the time\nWe use Bayes‚Äô Theorem to calculate the probability that a person actually has the disease given a positive test result.\nBayes‚Äô Theorem Explanation\nBayes‚Äô Theorem is used to update probabilities based on new evidence. It calculates the probability of an event (having the disease) given that another event has occurred (testing positive).\n\n# Define random variable for having the disease (1: Has disease, 0: No disease)\nD = Bernoulli('D', 0.01)  # Prior probability of disease (1%)\n\n# Define test results based on whether a person has the disease\nT_given_D = 0.95  # Sensitivity: P(T = 1 | D = 1) -&gt; True Positive Rate\nT_given_not_D = 0.05  # False Positive Rate: P(T = 1 | D = 0)\n\n# Total probability of testing positive\nP_T = T_given_D * P(Eq(D, 1)) + T_given_not_D * P(Eq(D, 0))\n\n# Compute P(D=1 | T=1) using Bayes' Theorem\nP_D_given_T = (T_given_D * P(Eq(D, 1))) / P_T\n\nprint(\"Probability of actually having the disease given a positive test:\", P_D_given_T)\n\nProbability of actually having the disease given a positive test: 0.161016949152542\n\n\nExample 4: Grading System Using a Normal Distribution\nA professor assigns grades based on a normal distribution of student scores, with: - Mean = 70 - Standard deviation = 10\nThe grading scale is as follows: - A: 85 and above - B: 70 to 84 - C: 50 to 69 - F: Below 50\nWe will calculate the no of students getting a particular grade assuming a class size of 100.\n\nmean = 70\nstd_dev = 10\ntotal_students = 100\n\n# Define the normal distribution\ngrades = Normal('G', mean, std_dev)\n\n# Compute the fraction and number of students in each grade category\nP_A = norm.sf(85, mean, std_dev)   # P(X &gt;= 85)\nP_B = norm.cdf(84, mean, std_dev) - norm.cdf(70, mean, std_dev)  # P(70 ‚â§ X ‚â§ 84)\nP_C = norm.cdf(69, mean, std_dev) - norm.cdf(50, mean, std_dev)  # P(50 ‚â§ X ‚â§ 69)\nP_F = norm.cdf(50, mean, std_dev)  # P(X &lt; 50)\n\n\nstudents_A = int(P_A * total_students)\nstudents_B = int(P_B * total_students)\nstudents_C = int(P_C * total_students)\nstudents_F = int(P_F * total_students)\n\n\nprint(\"Number of students getting an A:\", students_A)\nprint(\"Number of students getting a B:\", students_B)\nprint(\"Number of students getting a C:\", students_C)\nprint(\"Number of students getting an F:\", students_F)\n\n\n#  Visualization \nx = np.linspace(30, 110, 500)\ny = norm.pdf(x, mean, std_dev)\n\nplt.figure(figsize=(10, 5))\nplt.plot(x, y, color='blue', label=\"Normal Distribution (Œº=70, œÉ=10)\")\n\nplt.fill_between(x, y, where=(x &gt;= 85), color='green', alpha=0.4, label=f\"A (85+) - {students_A} students\")\nplt.fill_between(x, y, where=((x &gt;= 70) & (x &lt; 85)), color='blue', alpha=0.4, label=f\"B (70-84) - {students_B} students\")\nplt.fill_between(x, y, where=((x &gt;= 50) & (x &lt; 70)), color='orange', alpha=0.4, label=f\"C (50-69) - {students_C} students\")\nplt.fill_between(x, y, where=(x &lt; 50), color='red', alpha=0.4, label=f\"F (&lt;50) - {students_F} students\")\n\n\nplt.xlabel(\"Score\")\nplt.ylabel(\"Probability Density\")\nplt.title(\"Grading Distribution (Normal Curve)\")\nplt.legend(loc=\"upper right\", fontsize=10, frameon=True)\nplt.grid()\nplt.show()\n\nNumber of students getting an A: 6\nNumber of students getting a B: 41\nNumber of students getting a C: 43\nNumber of students getting an F: 2\n\n\n\n\n\n\n\n\n\n\n\nUse Cases\nPractical Applications\nSympy(Python library for symbolic mathematics) allows exact computations unlike numerical methods that approximate values and it is used in solving some real world problems. Here are some of the practical usecases:-\n\nCryptography & Cybersecurity\n\n\n\nHandling modular arithmetic\nImplementing RSA-related computations\n\n\nMedicine & Drug Discovery\n\n\nHelps in predicting how fast a disease spreads\n\n\nFinance & Economics\n\n\nHelps find the best investment allocation mathematically!\n\n\nCalculus (Differentiation & Integration)\n\n\nEvaluating limits\nComputing derivatives and integrals\nHandling series expansions\n\n\nPhysics and Mechanics\n\n\nSymbolic solutions for equations of motion\nComputing work and energy expressions\n\n\nControl Systems\n\n\nLaplace transforms\nTransfer functions\n\n\n\nConclusion\nSymPy Stats is a powerful tool for symbolic probability computations, making it valuable for theoretical and educational purposes. Unlike numerical approaches, it enables exact probability manipulations, making it particularly useful in research, teaching, and analytical problem-solving.\nWith its ability to define and manipulate random variables algebraically, compute probabilities, expectations, variances, and even perform conditional probability analysis, SymPy Stats stands out as a unique library. It provides a deeper understanding of probability theory by allowing symbolic derivations instead of just numerical approximations.\nAdditionally, integrating SymPy Stats with visualization tools like Matplotlib enhances comprehension by allowing users to visualize probability distributions and their properties effectively. Whether for academic study, probabilistic modeling, or symbolic computations, SymPy Stats serves as a versatile and robust library in the Python ecosystem.\n\n\nReferences & Further Reading\nOfficial Documentation\nSymPy Stats Documentation: Sympy Stats\nSymPy Official Website: Sympy\nTutorials\nPython SymPy Tutorial ‚Äì Introduction to the Symbolic Computation Library - YouTube"
  },
  {
    "objectID": "index.html",
    "href": "index.html",
    "title": "Sympy Stats",
    "section": "",
    "text": "Visit the Blog secction!"
  }
]